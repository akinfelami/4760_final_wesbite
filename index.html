<html>

<head>
    <title>MacroData Refinement on Pico - ECE 4760 Final Report</title>
    <meta name="description"
        content="A ChatGPT user interface for music generation with a keypad connected to an RP2040. ECE 4760 Final Project by Akinfolami Akin-Alamu and Wilson Coronel.">
    <meta name="keywords" content="RP2040, ChatGPT, music generation, ECE 4760, embedded systems, Pico">
    <meta name="author" content="Akinfolami Akin-Alamu, Wilson Coronel">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta content="text/html; charset=UTF-8" http-equiv="content-type">
    <style type="text/css">
        @import url(https://themes.googleusercontent.com/fonts/css?kit=lhDjYqiy3mZ0x6ROQEUoUw);
        @import url('https://fonts.googleapis.com/css2?family=Source+Sans+Pro:wght@400;500;600;700&display=swap');

        ul {
            list-style-type: disc;
            margin-bottom: 1em;
            line-height: 2;
        }

        table td,
        table th {
            padding: 0
        }

        .title {
            padding-top: 0pt;
            color: #000000;
            font-size: 26pt;
            padding-bottom: 3pt;
            line-height: 1.15;
            page-break-after: avoid;
            orphans: 2;
            widows: 2;
            text-align: justify
        }

        .subtitle {
            padding-top: 0pt;
            color: #666666;
            font-size: 15pt;
            padding-bottom: 16pt;
            line-height: 1.15;
            page-break-after: avoid;
            orphans: 2;
            widows: 2;
            text-align: left
        }

        li {
            color: #000000;
            font-size: 11pt;
        }

        p {
            line-height: 1.5;
        }

        h1 {
            text-align: left;
        }

        h2 {
            text-align: left
        }

        h3 {
            text-align: left
        }

        h4 {
            text-align: left
        }

        code {
            font-family: "Consolas";
            background-color: #1e1e1e;
            color: #d4d4d4;
            padding: 2px 4px;
            border-radius: 3px;
            font-size: 10.5pt;
        }

        body {
            font-family: "Source Sans Pro", sans-serif;
            text-align: justify;
            font-size: 12pt;
            width: 100%;
            margin-left: auto;
            margin-right: auto;
            margin-top: auto;
            margin-bottom: auto;
            padding: 50px 20px;
            max-width: 768px;
            box-sizing: border-box;
            background-color: #f8f7f6;
        }
    </style>
</head>

<body>
    <h1>MacroData Refinement on Pico</h1>
    <h2>ECE 4760 Final Report</h2>
    <h3>Akinfolami Akin-Alamu (aoa9) &amp; Wilson Coronel (wrc62)</h3>
    <h2>Introduction</h2>
    <p>Sound Bite: We demonstrate a ChatGPT user interface for music generation with a
        keypad connected to an RP2040.</p>
    <p></p>
    <p>Summary: In this
        project, we successfully built an audio synthesizer that plays musical melodies of various lengths and
        styles generated by ChatGPT. The user has control over the key the melody is in via presses of a keypad, as
        well as control over the length and style of the generated melody through simple edits to the prompt. Our
        implementation consists of a python file that uses the ChatGPT API to produce the melodies (run on the host
        computer), and a C file that runs on the RP2040 and communicates with the python file by sending the key
        pressed to the host and receives the melody from the host. </span></p>
    <h2>High level design</h2>
    <p>The primary inspiration for this
        project was the television series Severance. A central element of the show is Lumon
        Industries' Macro Data Refinement department, where "severed" employees sort groups of numbers based on the
        emotions
        (Woe, Frolic, Dread, and Malice) these numbers evoke. The "terminal-looking" interface provided a compelling
        basis for
        an interactive game. We also drew inspiration from the fan-made web version of the game available at
        lumon-industries.com, aiming to create a hardware-based counterpart. The project's goal was to not only
        replicate the
        game mechanics but also to weave in "easter eggs" and the general aesthetic of the show.</p>
    <p>We utilized fixed-point
        arithmetic for efficient computation, particularly within the boid animation and game logic
        updates. We used 16.15 fixed-point numbers for calculations involving boid positions, velocities, and
        behavioral
        parameters. This allowed us to cram heavy floating point computations into the game logic and still meet
        frame rate
        deadlines without sacrificing smooth animations. Additionally, besides computing positions of pixels on the
        screen, much
        of the math in this game came from the movement of the invisible "boids" that cause numbers to quiver. This
        movement is
        governed by a flocking algorithm which causes each boid to adjust its velocity based on factors such as:
        cohesion
        (moving towards the average position of nearby boids), separation (avoiding really close by boids) and wall
        avoidance
        (turn away from screen boundaries). More parameters such as VISUAL_RANGE, PROTECTED_RANGE control the
        specifics of these
        behaviors and distance calculations were approximated using the "Alpha max plus beta min" method.
    </p>
    <p>The game operates using a state machine and concurrent processing handled by
        protothreads.
    </p>
    <h2>Program/Hardware design</h2>
    <p>The Program to Hardware connections are as follows: </p>
    <ul>
        <li>the Host (Computer I/O) waits for a keypad input from the
            keypad to be processed in the RP2040.</li>
        <li>the RP2040 processes the keypad input and assigns a musical key
            to each keypad key (here there are two definitions of the word &ldquo;key&rdquo;)&mdash; 2 corresponds
            to A major, 3 to B major, 4 to C major, etc.</li>
        <li>Depending on which key is pressed, the RP2040 sends a message
            to the host through UART which then gets sent to ChatGPT as a prompt.</li>
        <li>After creating a 10 note melody in the designated key, the host
            sends back a serial signal to the RP2040 .</li>
        <li>The RP2040 then processes this array as in the format of a 20
            length array of [frequencies, durations].</li>
        <li>The RP2040 then plays the 10 notes through the PWM and outputs
            on the speakers.</li>
    </ul>
    <h2>Hardware Setup</h2>
    <div style="display: flex; flex-direction: column; align-items: center;">
        <img src="images/image3.jpg" alt="Hardware Setup">
        <p>Figure 1: Hardware Setup</p>
    </div>
    <div style="display: flex; flex-direction: column; align-items: center;">
        <img src="images/image1.jpg" alt="Hardware Setup (top view)">
        <p>Figure 2: Hardware setup (top view)</p>
    </div>
    <p>In Figures 1 and 2, we can see the hardware implementation including the keypad on the
        bottom left where we send input signals to the DAC. It is almost identical in setup to the Bird Sound
        Synthesis lab &ndash; it is connected to the RP2040 which is located on the right and connected to the lab
        computer as well as the serial jack (seen in the blue), and the audio jack which feeds into a speaker via a
        3.5mm audio socket that outputs generated beeps. The DAC is connected (to send SPI signals) to the Pico by
        connecting each of the ports following the DAC datasheet and the #define statements in the source code.
    </p>
    <p>The DAC pins are VDD, chip select, the clock line, SDI (same as MOSI), VoutA (one of
        the outputs), VSS is ground, VOUTB (the other output) and the LDAC pin. The LDAC pin allowed us to
        separately load the left and right channels for the DAC and then, when that pin is toggled, both are sent to
        the output simultaneously.</p>
    <p>A similar procedure was done for the 3.5mm audio
        socket, which had two pin configurations, not including its GND of course. </p>
    <div style="display: flex; flex-direction: column; align-items: center;">
        <img src="images/image2.png" alt="Diagram of Hardware Setup">
        <p>Figure 3: Diagram of Hardware Setup</p>
    </div>
    <h2>Python ChatGPT Script</h2>
    <p>Because the ChatGPT API is a python library that can easily be imported, we chose to have
        one file dedicated to solely running all of the ChatGPT-related calls. In our python script, run by the host
        computer, we modularized our code into 3 primary functions, one that uses the
        openai&nbsp;library to prompt ChatGPT and receive its response, one
        that corresponds each musical note into its corresponding frequency that can be output by the Pico, and one
        that cleans and processes the response from ChatGPT. </p>
    <p>In the
        chat_with_gpt_for_melody(note_key) function, we include our personal OpenAI API key, our prompt, and the
        ChatCompletion.create(model=&rdquo;gpt-4&rdquo;, messages=[{&ldquo;role&rdquo;: &ldquo;system&rdquo;,
        &ldquo;content&rdquo;: &ldquo;You are a helpful music assistant.&rdquo;}, {&ldquo;role&rdquo;:
        &ldquo;user&rdquo;, &ldquo;content&rdquo;: prompt}]) which asks ChatGPT to give us our melody output. This
        function is highly customizable, but we mainly wanted to demonstrate the baseline concept of using the
        ChatGPT API in this project. The prompt that worked most consistently for us was:</p>
    <p>&quot;Create a short melody of 10 notes in the key of {note_key}. Make sure
        that the first note is the {note_key}. Provide the notes and their durations in a list. Example format:
        [A4,500,B4,300,C5,400]&quot;.</p>
    <p>Note_key here is the parameter that is passed in from the C file that handles
        the Pico key press, and we include that here in our prompt. We realized that if we did not include the
        example format, ChatGPT would not reliably always output the array that we ultimately wanted. The output of
        this function contains the entire output from ChatGPT, including the key piece of information which is a
        list of music notes and their corresponding durations in milliseconds in the immediate next element of the
        list (e.g. [A4, 400, B4, 500, F5, 250]). We tried including things in our prompt that would cause ChatGPT to
        output melodies in different styles (e.g. Christmas) or different clefs (e.g. treble or base), and it was
        able to generate melodies that matched such modifications to our prompt, but not entirely reliable every
        time (more qualitative than quantitative as well). </p>
    <p>The next function
        note_to_frequency(note) simply contains a dictionary of notes (e.g. A4, B4, C#5) as the keys and their
        corresponding frequencies as the values. We only included 22 notes here out of at least 88 possible keys (of
        the piano) for simplicity. The function would take in the parameter of the note, and return its
        corresponding value from the dictionary.</p>
    <p>The
        clean_melody_response(response) function is necessary because everytime ChatGPT outputs a response, it
        contains a lot of extraneous information (to act more like a chatbot rather than a search engine) besides
        the array of music notes and durations that we ultimately want. Thus, we need to go through each character
        of the output and only keep the information contained within square brackets (e.g. [A4, 400, B4, 500, F5,
        250]). Afterwards, we use the note_to_frequency function to convert every other element of the list
        (starting from the first element) into its corresponding 3-character frequency.</p>
    <p>In our main function, we have one continuously
        running while loop that is constantly listening for incoming data from the Pico (which is the key press).
        Once it receives the data via serial (using ser.readline() where ser is imported
        pyserial&nbsp;library), it calls the chatgpt function to get the
        melody, cleans the melody using the clean melody function, and then writes the cleaned melody in the form of
        a string, back to the RP2040 using ser.write(melody.encode()). Because we are using one COM port to
        communicate through from both the python file (host computer) and the C file (Pico), we can&rsquo;t use
        PuTTy to see what is being printed out at each step of the pipeline, we chose to use the python file to
        print out everything that the user sees, such as what the melody list produced by GPT and played by the Pico
        looks like, or what key they pressed. </p>
    <h2>C Script</h2>
    <p>In our C script, we have 2
        protothreads, one that is responsible for scanning the keypad and sending the pressed key over to the host
        computer, and one that is responsible for parsing and playing the melody. Most of the libraries that we need
        are kept from Lab1, with the exception of defining 2 PWM pins (18 &amp; 19) and PWM parameters such as
        CLKDIV, slice_num, and frequency.</p>
    <p>Our first thread contains a
        while loop that is always running (we can physically see this with a blinking LED) and a for loop that is
        scanning the entire keypad continuously until a button is pressed and the musical note corresponding to that
        button is sent to the host computer for ChatGPT processing. Once a button is pressed, we look for a valid
        keycode (12 possibilities on the keypad) and if there exists one (both button, keycodes[12], and
        scancodes[4] are unsigned global integer variables) then we proceed to the logic that determines which
        musical note key to be sent to the python file. If the current key does not equal the previous key and
        State0 equals 0 (default is that State0 = 0), then we find the if branch that matches the key that we
        pressed (e.g. 1, 2, 3,..., 7) and within the conditional branch, call uart_puts(uart0, &ldquo;major
        key\n&rdquo;) and change State0 to 1. The uart_puts function sends the string containing the note key
        pressed to the python file, which can then be included in the prompt sent to ChatGPT. The purpose of
        changing the volatile unsigned int State0 is so that thread 2 knows when to execute the parsing and playing
        of the melody whenever State0 is equal to 1 (thread 2 doesn&rsquo;t execute if State0 is equal to 0).
        Ultimately, setting the variable of State0 did not end up mattering when we moved away from the ISR,
        especially since thread 2 wouldn&rsquo;t execute anyway until data is received from the python file, which
        only occurs after thread 1 sends the note pressed.</p>
    <p>The second protothread that we have running at
        all times is used to receive the output from ChatGPT via serial_read, parse the list of frequencies and
        durations into separate arrays containing just the frequencies and just the durations, and then play the
        melody using PWM. We have 2 global int arrays defined as frequencies[] and durations[] of fixed length 10 or
        20 (size of array depends on how many notes we want ChatGPT to generate). The static variables that we
        maintain within this thread are int note_count = 0, bool is_frequency = true, and char *token. We first call
        serial_read which waits for serial data to be read, and then we check if data is received in the
        pt_serial_in_buffer and if State0 equals 1. If data is received, we parse the received string token by
        token, separated by commas. If the current token isn&rsquo;t NULL and the note_count is less than the size
        of the list (if there are 10 notes played, then there are 10 frequencies and 10 durations in the list, so
        the size will be 20), then we store the values of each token in the frequencies and durations lists. The
        first token stored will always be in the frequencies list, and the second token will always be in the
        durations list, and this pattern will alternate until a NULL
        character is found at the end (thus the size of the list will always be an even number). To handle storing
        in different lists each time, we have a simple boolean flag is_frequency that controls such execution: if it
        is true then the note will be stored in the frequencies list and is_frequency is set to false, if it is
        false then the note will be stored in the durations list and is_frequency is set to true. The loop ends when
        the current token from the buffer is a null character, indicating the end of the list. Once we have both of
        our lists, we have a for loop that loops through both lists and plays the melody. We call
        pwm_set_wrap(slice_num, WRAPVAL) and then pwm_set_clkdiv(slice_num, CLKDIV) where CLKDIV uses each element
        of our frequencies list. We set the channel level of one of our PWM ports to be 2500 and enable the mask to
        play a note. This note will play for the amount of time defined in each element of our durations list (using
        sleep_ms). Then the note stops playing by calling the same PWM functions as before (except enabling the
        mask) but setting the channel level of the PWM port to be 0 causes the note to stop playing. We also add a
        100 ms delay between each note being played.</p>
    <p>In the main function, we simply
        keep all of the relevant initializations for UART, PWM, GPIOs, etc. and add our 2 threads. Whenever we want
        to demo our project or test to see if the entire process works, we simply build the CMake files (same as lab
        1 with the exception of adding PWM), move the beep_beep uf2 file into our Pico, and then run the ChatGPT
        python file. The user will be asked to press a key on the keypad, where the prompt explains that pressing 1
        equals A major, 2 equals B major, etc. Once the key is pressed, there will be a few seconds (under 5
        seconds) of waiting time for ChatGPT to generate its response, and then the melody can be heard on the
        speakers.</p>
    <h2>Testing &amp; Fixes</h2>
    <p>There were many issues that we
        encountered throughout our implementation and overcame. Firstly, we tried to use an ISR for DDS to output
        the melodies, however, we would always run into concurrency issues that stemmed from the ISR. Sometimes, the
        melody would not be played at all and an infinite loop would result; other times only 3 or 4 notes of the
        melody would be played. We think these weird behaviors may have been caused by our lack of a well-defined
        debouncing machine, even though we did not really need one since the user only presses the keypad once for a
        melody to be played and if the user chooses to press the keypad multiple times, only the first key press
        will be registered. We thought that the ISR might be interfering with our sending data back and forth
        between the Pico and the host computer, but print statements showed that data was indeed being sent and
        received. Because debugging the sound output was tedious, we decided to try out PWM instead, which only
        required two additional GPIO pins to set up and did not need any ISR or DDS. This way we can include our
        logic for playing the melody within one of our protothreads directly, which makes the multithreading issues
        much simpler to resolve. By using PWM, we immediately had a successful output of the melody through the
        speakers and it would work just about every time without fail.</p>
    <p>Previously, instead of using
        serial_read to get the entire string from the python file, we tried using uart_getc (there was no method
        called uart_gets, which would supposedly get the entire string much like uart_puts puts the entire string to
        the serial connection) which gets every character individually from the buffer. This method was not only
        much more tedious, increasing the time complexity, but also did not work reliably due to concurrency issues.
        We tried storing the tokens from uart_getc in both a character array as well as an integer array, but both
        could not resolve these issues.</p>
    <p>Another issue that we faced was
        the prompting of ChatGPT being volatile, since the chatbot would not reliably output what we wanted every
        single time. We also found that our current prompt doesn&rsquo;t work so well with very few notes (&lt;3) or
        too many notes (20&lt;), so modifications to the prompt will need to be made to accommodate these
        changes.</span></p>
    <h2>Results of the design</h2>
    <p>We were successful in having
        ChatGPT produce an interesting melody in the key that we pressed and we verified through using the
        oscilloscope and an iPhone app that the frequencies that were being output corresponded to the correct
        notes. Overall, the first note that was produced in the melodies was the key that we pressed, which we
        wanted. The rest of the notes would be in the key of the note that we pressed and the number of notes in the
        melody would always correspond to the number of notes we specified in the prompt to ChatGPT. The melodies
        themselves would have arpeggios and some scales (consecutive notes up or down the measure) and sounded
        pleasant.</p>
    <p>The speed of the execution is
        almost entirely dependent upon how quickly ChatGPT would be able to generate the melody based on our prompt.
        Thus, we focused on making our prompt as reliable and concise as possible so that ChatGPT would spend just a
        few seconds to produce the melody. Otherwise, there is very low latency between the time when the user
        presses the keypad to when sound is output through the speakers. After switching to PWM, we no longer faced
        any concurrency issues that hindered us greatly in our DDS implementation. </p>
    <p>Our project has no safety concerns that we can think of. The project is
        highly usable for anyone since all that is required to generate melodies</p>
    <h2>Conclusions</h2>
    <p>Overall, we were proud of our
        final project and ultimately succeeded in having ChatGPT generate different melodies each time a new key is
        pressed. Our expectations since the start of the project were met, although there are many improvements that
        can be made. I think our structure of having a python file in charge of using the ChatGPT API and a C file
        that handles the Pico controls, and communicating over UART and Serial works well with low latency. We were
        initially worried that the prompting of ChatGPT would be too volatile and would not output the correct array
        of frequencies and durations (or even an array at all), but after thorough prompt engineering, the output
        from ChatGPT works almost 100% of the time. Our testing methodology of using the python script to print out
        what is going on at every stage of the pipeline, whether that&rsquo;s in the protothreads or in the python
        function, was crucial in our debugging.</p>
    <p>Given more time, w</span><span>e wanted to completely finish adding a record mode by
            pressing 0 on the keypad, where many keys can be pressed (until the 0 key is pressed again), so that we can
            have an initial melody of a few notes that chatgpt can then take to create an even more elaborate melody,
            instead of creating a melody solely based on one key press.</span><span class=" ">&nbsp;We can then
            save whatever melody was generated in some database file so that if the user wanted to play it back in the
            future, they could do so easily. If such data was stored, there could be much more elaborate
            songs/compositions that can be constructed by GPT, just by simply prompting it with multiple melodies that
            it had prompted earlier. We wanted to push the limits to see if it can generate different harmonies (chords
            perhaps) instead of just a singular melody</span></p>
    <p>We adapted our C code from lab 1 due to its similarity with what we were trying to achieve,
        although our code differs due to additional protothreads, using PWM instead of an ISR for DDS, not using a
        VGA display, and overall logic. We did not use any of Altera&rsquo;s IP, did not code in the public domain,
        did not need to deal with patent/trademark issues, and did not have to sign a non-disclosure agreement to
        get a sample part. There are certainly a myriad of future possibilities that combine LLMs with embedded
        devices that may lead to patent opportunities.</span></p>
    <h2>Appendix A</h2>
    <ul>
        <li>&quot;The group approves this report for inclusion on the
            course website.&quot;</li>
        <li>&quot;The group approves the video for inclusion on the course
            youtube channel.&quot;</li>
    </ul>
    <h2>Additional Appendices</h2>
    <h3>Primary Task Distribution</h3>
    <p>Both members contributed to all aspects of the project, these were the focuses
        of each team member:</p>
    <ul>
        <li>Edward: Ideation of project, ChatGPT python script, C script,
            debugging</li>
        <li>Filipe: Building the hardware setup, C script, debugging</li>
        </li>
    </ul>
    <h2>References</h2>
    <ul>
        <li>We used the ChatGPT API documentation to help us reliably
            communicate with ChatGPT through the terminal.</li>
        <li>We used the RP2040 datasheets and Lab1 as references.</li>
        </li>
        <li>Our PWM implementation is adapted from a fellow student,
            Pelham, who was also having trouble with DDS and was successful in using PWM.</li>
        <li>All materials used are from Lab1.</li>
    </ul>
</body>

</html>