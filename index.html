<html>

<head>
    <title>MacroData Refinement - Severance-Inspired Number Sorting Game</title>
    <meta name="description"
        content="A Severance-inspired MacroData Refinement implementation where numbers are sorted based on emotions (Woe, Frolic, Dread, and Malice) they evoke. Built on Raspberry Pi Pico.">
    <meta name="keywords" content="Severance, MacroData Refinement, Lumon Industries, Raspberry Pi Pico, number sorting, emotions, Woe, Frolic, Dread, Malice">
    <meta name="author" content="Akinfolami Akin-Alamu, Wilson Coronel">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta content="text/html; charset=UTF-8" http-equiv="content-type">
    <style type="text/css">
        @import url(https://themes.googleusercontent.com/fonts/css?kit=lhDjYqiy3mZ0x6ROQEUoUw);
        @import url('https://fonts.googleapis.com/css2?family=Source+Sans+Pro:wght@400;500;600;700&display=swap');

        ul {
            list-style-type: disc;
            margin-bottom: 1em;
            line-height: 2;
        }

        li {
            color: #000000;
        }

        p {
            line-height: 1.5;
        }

        h1 {
            text-align: left;
        }

        h2 {
            text-align: left
        }

        h3 {
            text-align: left
        }

        h4 {
            text-align: left
        }

        pre {
            background: #f4f4f4;
            border: 1px solid #ddd;
            border-left: 3px solid #f36d33;
            color: #666;
            page-break-inside: avoid;
            font-family: monospace;
            font-size: 15px;
            line-height: 1.6;
            margin-bottom: 1.6em;
            max-width: 100%;
            overflow: auto;
            padding: 1em 1.5em;
            display: block;
            word-wrap: break-word;
        }

        code {
            background: #f4f4f4;
            border: 1px solid #ddd;
            border-radius: 3px;
            color: #333;
            font-family: Consolas, Monaco, 'Andale Mono', monospace;
            font-size: 0.9em;
            padding: 0.2em 0.4em;
            margin: 0 0.2em;
            white-space: nowrap;
            overflow-x: auto;
            vertical-align: middle;
        }

        body {
            font-family: "Source Sans Pro", sans-serif;
            text-align: justify;
            font-size: 12pt;
            width: 100%;
            margin-left: auto;
            margin-right: auto;
            margin-top: auto;
            margin-bottom: auto;
            padding: 50px 20px;
            max-width: 768px;
            box-sizing: border-box;
            background-color: #f8f7f6;
        }
    </style>
</head>

<body>
    <h1>MacroData Refinement on Pico</h1>
    <h3>ECE 4760 Final Report</h3>
    <h3>Akinfolami Akin-Alamu (aoa9) &amp; Wilson Coronel (wrc62)</h3>
    <h2>Introduction</h2>
    <!-- <p>Sound Bite: We demonstrate a ChatGPT user interface for music generation with a
        keypad connected to an RP2040.</p>

    <p>Summary: In this
        project, we successfully built an audio synthesizer that plays musical melodies of various lengths and
        styles generated by ChatGPT. The user has control over the key the melody is in via presses of a keypad, as
        well as control over the length and style of the generated melody through simple edits to the prompt. Our
        implementation consists of a python file that uses the ChatGPT API to produce the melodies (run on the host
        computer), and a C file that runs on the RP2040 and communicates with the python file by sending the key
        pressed to the host and receives the melody from the host. </span></p> -->
    <h2>High level design</h2>
    <p>The primary inspiration for this
        project was the television series Severance. A central element of the show is Lumon
        Industries' Macro Data Refinement department, where "severed" employees sort groups of numbers based on the
        emotions
        (Woe, Frolic, Dread, and Malice) these numbers evoke. The "terminal-looking" interface provided a compelling
        basis for
        an interactive game. We also drew inspiration from the fan-made web version of the game available at
        lumon-industries.com, aiming to create a hardware-based counterpart. The project's goal was to not only
        replicate the
        game mechanics but also to weave in "easter eggs" and the general aesthetic of the show.</p>
    <p>We utilized fixed-point
        arithmetic for efficient computation, particularly within the boid animation and game logic
        updates. We used 16.15 fixed-point numbers for calculations involving boid positions, velocities, and
        behavioral
        parameters. This allowed us to cram heavy floating point computations into the game logic and still meet
        frame rate
        deadlines without sacrificing smooth animations. Additionally, besides computing positions of pixels on the
        screen, much
        of the math in this game came from the movement of the invisible "boids" that cause numbers to quiver. This
        movement is
        governed by a flocking algorithm which causes each boid to adjust its velocity based on factors such as:
        cohesion
        (moving towards the average position of nearby boids), separation (avoiding really close by boids) and wall
        avoidance
        (turn away from screen boundaries). More parameters such as VISUAL_RANGE, PROTECTED_RANGE control the
        specifics of these
        behaviors and distance calculations were approximated using the "Alpha max plus beta min" method.
    </p>
    <p>The game operates using a state machine and concurrent processing handled by
        protothreads.
    </p>
    <ul>
        <li><strong>Game States:</strong> The game progresses through three states defined by the
            <code>PlayState</code> enum:
            The game begins in the <code>START_SCREEN</code> state, which displays an initial message prompting the user
            to
            start. During the <code>PLAYING</code> state, which is the main interactive phase, the refinement process
            takes
            place. Finally, when all refining is complete, the game transitions to the <code>GAME_WON</code> state where
            it
            displays the completion screen.
        </li>
        <li><strong>Main loop:</strong> Within the <code>PLAYING</code> state, the system continuously.
            It updates boid positions and velocities, checks for collisions between boids and the
            number grid (which triggers animations), and scans for user input through the joystick for cursor movement
            and
            button for refinement. When refinement is triggered on a "bad number", the game state is updated by removing
            refined numbers and regenerating new ones, bin animations are triggered, and the overall progress bar is
            updated. Finally, all visual elements are redrawn on the VGA screen.
        </li>
        <li><strong>Protothreads:</strong> Concurrency is managed using the
            <code>pt_cornell_rp2040_v1_3.h</code> library.
            On Core 0, we have three protothreads: protothread_graphics which handles drawing the main game screen,
            protothread_joystick which reads ADC values from the joystick and updates cursor position, and
            protothread_button_press which debounces the refinement button and triggers refinement logic. Core 1 runs
            two
            protothreads: protothread_graphics_too which manages the animations of the Woe, Frolic, Dread, and Malice
            bins,
            and protothread_progress_bar which updates the state of the main progress bar animation.
        </li>

    </ul>
    <p>
        All the game code is written in C. We utilized the provided <a
            href="https://github.com/vha3/Hunter-Adams-RP2040-Demos/tree/master/VGA_Graphics">vga library</a> which gave
        us a great head start. An Analog
        joystick (purchased from Amazon) was chosen for user interaction. The proposal initially considered eight
        buttons, but a
        joystick for cursor movement was found to be more intuitive for navigating the grid.
    </p>
    <h3>Existing Patents, Copyrights, and Trademarks</h3>
    <p>This project is directly inspired by the Severance television show and related imagery from Lumon Industries,
        which are
        intellectual property of Apple Inc. and/or the show's creators. Our project is intended solely for educational
        and
        non-commercial purposes within the context of ECE 4760. We do not claim ownership of any copyrighted or
        trademarked
        material from Severance. The game mechanics, while inspired by the show, are our own faithful interpretation and
        implementation.</p>
    <h2>Program/Hardware design</h2>
    <!-- <p>The Program to Hardware connections are as follows: </p>
    <ul>
        <li>the Host (Computer I/O) waits for a keypad input from the
            keypad to be processed in the RP2040.</li>
        <li>the RP2040 processes the keypad input and assigns a musical key
            to each keypad key (here there are two definitions of the word &ldquo;key&rdquo;)&mdash; 2 corresponds
            to A major, 3 to B major, 4 to C major, etc.</li>
        <li>Depending on which key is pressed, the RP2040 sends a message
            to the host through UART which then gets sent to ChatGPT as a prompt.</li>
        <li>After creating a 10 note melody in the designated key, the host
            sends back a serial signal to the RP2040 .</li>
        <li>The RP2040 then processes this array as in the format of a 20
            length array of [frequencies, durations].</li>
        <li>The RP2040 then plays the 10 notes through the PWM and outputs
            on the speakers.</li>
    </ul>
    <h2>Hardware Setup</h2>
    <div style="display: flex; flex-direction: column; align-items: center;">
        <img src="images/image3.jpg" alt="Hardware Setup">
        <p>Figure 1: Hardware Setup</p>
    </div>
    <div style="display: flex; flex-direction: column; align-items: center;">
        <img src="images/image1.jpg" alt="Hardware Setup (top view)">
        <p>Figure 2: Hardware setup (top view)</p>
    </div>
    <p>In Figures 1 and 2, we can see the hardware implementation including the keypad on the
        bottom left where we send input signals to the DAC. It is almost identical in setup to the Bird Sound
        Synthesis lab &ndash; it is connected to the RP2040 which is located on the right and connected to the lab
        computer as well as the serial jack (seen in the blue), and the audio jack which feeds into a speaker via a
        3.5mm audio socket that outputs generated beeps. The DAC is connected (to send SPI signals) to the Pico by
        connecting each of the ports following the DAC datasheet and the #define statements in the source code.
    </p>
    <p>The DAC pins are VDD, chip select, the clock line, SDI (same as MOSI), VoutA (one of
        the outputs), VSS is ground, VOUTB (the other output) and the LDAC pin. The LDAC pin allowed us to
        separately load the left and right channels for the DAC and then, when that pin is toggled, both are sent to
        the output simultaneously.</p>
    <p>A similar procedure was done for the 3.5mm audio
        socket, which had two pin configurations, not including its GND of course. </p>
    <div style="display: flex; flex-direction: column; align-items: center;">
        <img src="images/image2.png" alt="Diagram of Hardware Setup">
        <p>Figure 3: Diagram of Hardware Setup</p>
    </div> -->
    <h3>Overall Architecture</h3>
    <p>
        The system leverages both cores of the RP2040. Core 0 is responsible for the main graphics rendering
        (<code>protothread_graphics</code>), joystick input (<code>protothread_joystick</code>), and the refinement
        button logic
        (<code>protothread_button_press</code>). Core 1 handles animations: the bottom bins' grow/shrink effect
        (<code>protothread_graphics_too</code>)
        and the progress bar fill animation logic (<code>protothread_progress_bar</code>). This division distributes the
        workload,
        allowing
        for smoother animations and responsive input. Communication between threads, such as signaling the start of the
        game, is
        managed using semaphores (<code>start_game_sem</code>).
    </p>
    <h3>Game State Management</h3>
    <p>The central data structure is <code>GameState</code>, defined in <code>game_state.h</code>. It
        covers all dynamic aspects of the game:</p>
    <p>The game state consists of several key components: <code>state[ROWS][COLS]</code> is a 2D array of Number structs
        representing the grid of numbers, where each Number stores its value, position, size, animation flags
        (animated_last_frame_by_boid0/1, refined_last_frame), and whether it's a "bad number" (is_bad_number) along with
        its associated BadNumber data (target bin_id). <code>boids[NUM_BOIDS]</code> is an array of Boid structs,
        storing their position (x, y), velocity (vx, vy), bias, and scout group. <code>box_anims[5]</code> is an array
        of BoxAnim structs that manages the state of the animated bins at the bottom, including their current animation
        height, state (ANIM_IDLE, ANIM_GROWING, ANIM_SHRINKING), and the percentage of each "emotion" (Woe, Frolic,
        Dread, Malice) they contain. The <code>cursor</code> is a Cursor struct storing the position and dimensions of
        the player-controlled cursor. <code>play_state</code> is an enum PlayState indicating the current phase of the
        game (e.g., START_SCREEN, PLAYING). <code>total_bad_numbers</code> tracks the count of unrefined "bad numbers" -
        when there are zero left, the game is won! Finally, <code>progress_bar</code> is a ProgressBarAnimation struct
        for managing the top progress bar's visual state.</p>

    <p>Initialization is handled by <code>game_state_init()</code>, which populates the number grid with random values,
        assigns some
        as "bad
        numbers" with random target bins, and spawns the boids. Spawning boids involves initializing all boids'
        position,
        velocity, and bias characteristics. Then, for each frame, we calculate new velocities and positions for each
        boid based
        on interactions with other boids and screen boundaries. Additionally, we check if a boid is within
        <code>BOID_COLLISION_RADIUS</code>
        of any number cell. If so, the <code>animate_numbers()</code> function is called, causing the number to slightly
        shift its
        position
        (quiver) based on a random pixel shift. If the number is a "bad number", its display size is increased. Flags
        (<code>animated_last_frame_by_boid0</code> or <code>animated_last_frame_by_boid1</code>) are set on the Number
        struct to indicate it has
        been
        "touched" by a boid. This is crucial for the refinement logic.
    </p>

    <h3>Number Grid and Refinement Logic</h3>
    <p>The game screen features a grid of numbers (<code>ROWS</code> x <code>COLS</code>). Numbers are initially random
        digits (0-9). A small
        percentage
        are flagged as <code>is_bad_number</code> and assigned a random <code>bin_id</code> (0-3) corresponding to one
        of the four emotion bins.
        When the
        player moves the cursor over a number and presses the refinement button, <code>handle_cursor_refinement()</code>
        function is
        invoked.
        This function identifies the number that is currently selected, using the current
        (<code>x</code>,<code>y</code>) position of the cursor.
        Then it
        checks if it is a bad number AND if it has been animated in the current frame. This is important so that we
        don't refine
        bad numbers that are not quivering in this frame. So if these conditions are met, the refined number and ALL the
        numbers
        that are animated by that same boid are marked as <code>refined_last_frame</code>. This is why we have a flag
        for each boid
        indicating it has animated some numbers. For example, when <code>boid0</code> comes in contact with some
        numbers, it marks
        them as
        <code>animated_last_frame_by_boid0</code>. This means that when we find that a bad_number is animated by the
        <code>boid0</code>, only
        numbers
        <code>animated_last_frame_by_boid0</code> are refined.
    </p>
    <p>Following, the identification of the numbers to be refined, <code>total_bad_numbers</code> is decremented, the
        corresponding
        bin
        animation (<code>state->box_anims[bin_id].anim_state = ANIM_GROWING;</code>) and the main progress bar animation
        are
        triggered.
        Finally, in the next main graphics cycle, numbers marked <code>refined_last_frame</code> are regenerated with
        new random
        values,
        potentially becoming new "bad numbers."</p>

    <p>A tricky part was coordinating the boid animation flags with the refinement logic to ensure only "quivering" bad
        numbers
        could be refined. It involved maintaining many layers of state between different parts of the game. The
        interaction
        where refining one number in a boid-activated cluster also "refines" the others in that cluster was an explicit
        design
        choice to simulate sorting a "group" of numbers as described in the show.</p>

    <h3>Graphics</h3>
    <p>Drawing is performed using the <code>vga16_graphics.h</code> library, which provides primitives for pixels,
        lines, rectangles,
        circles, and text. Our initial plan was to use a custom font. But we weren't able to find an open source font
        bitmap
        that correctly matched the given vga graphics library. We mostly used the provided functions like
        <code>drawPixel</code>,
        <code>drawLine</code>, <code>drawRect</code>, <code>fillRect</code>, <code>drawChar</code>,
        <code>setCursor</code>,
        <code>setTextColor</code>, <code>setTextSize</code> and <code>writeString</code>. We also implemented a
        <code>drawOval(short x0, short y0, short rx, short ry, char color)</code> method. This function uses the
        midpoint ellipse algorithm to draw
        ovals, which was essential for rendering the Lumon logo.
    </p>

    <p>The number quivering/sizing was by modifying <code>x</code>, <code>y</code>, <code>size</code> in the
        <code>Number</code> struct within <code>animate_numbers()</code> and
        redrawing in
        <code>protothread_graphics</code>. We made sure to cap the max pixel shift by the size of the current number
        grid. On the
        other hand,
        the <code>BoxAnim</code> struct's <code>current_anim_height</code> is incremented/decremented during
        <code>ANIM_GROWING</code>/<code>ANIM_SHRINKING</code> states,
        and
        <code>drawRect</code> is used to visualize this. The animation pauses for 3 seconds when fully grown to display
        percentages.
        This
        animation happens over more than one frame. Essentially, when the state is <code>ANIM_GROWING</code>, we
        progressively erase
        and draw
        the box taller than the last frame until it reaches a specific height after which we transition to
        <code>ANIM_SHRINKING</code>. In
        the next frame, the reverse of the process for growing the box happens.
    </p>
    <h2>Results of the design</h2>
    <p>We were successful in having
        ChatGPT produce an interesting melody in the key that we pressed and we verified through using the
        oscilloscope and an iPhone app that the frequencies that were being output corresponded to the correct
        notes. Overall, the first note that was produced in the melodies was the key that we pressed, which we
        wanted. The rest of the notes would be in the key of the note that we pressed and the number of notes in the
        melody would always correspond to the number of notes we specified in the prompt to ChatGPT. The melodies
        themselves would have arpeggios and some scales (consecutive notes up or down the measure) and sounded
        pleasant.</p>
    <p>The speed of the execution is
        almost entirely dependent upon how quickly ChatGPT would be able to generate the melody based on our prompt.
        Thus, we focused on making our prompt as reliable and concise as possible so that ChatGPT would spend just a
        few seconds to produce the melody. Otherwise, there is very low latency between the time when the user
        presses the keypad to when sound is output through the speakers. After switching to PWM, we no longer faced
        any concurrency issues that hindered us greatly in our DDS implementation. </p>
    <p>Our project has no safety concerns that we can think of. The project is
        highly usable for anyone since all that is required to generate melodies</p>
    <h2>Conclusions</h2>
    <p>Overall, we were proud of our
        final project and ultimately succeeded in having ChatGPT generate different melodies each time a new key is
        pressed. Our expectations since the start of the project were met, although there are many improvements that
        can be made. I think our structure of having a python file in charge of using the ChatGPT API and a C file
        that handles the Pico controls, and communicating over UART and Serial works well with low latency. We were
        initially worried that the prompting of ChatGPT would be too volatile and would not output the correct array
        of frequencies and durations (or even an array at all), but after thorough prompt engineering, the output
        from ChatGPT works almost 100% of the time. Our testing methodology of using the python script to print out
        what is going on at every stage of the pipeline, whether that&rsquo;s in the protothreads or in the python
        function, was crucial in our debugging.</p>
    <p>Given more time, w</span><span>e wanted to completely finish adding a record mode by
            pressing 0 on the keypad, where many keys can be pressed (until the 0 key is pressed again), so that we can
            have an initial melody of a few notes that chatgpt can then take to create an even more elaborate melody,
            instead of creating a melody solely based on one key press.</span><span class=" ">&nbsp;We can then
            save whatever melody was generated in some database file so that if the user wanted to play it back in the
            future, they could do so easily. If such data was stored, there could be much more elaborate
            songs/compositions that can be constructed by GPT, just by simply prompting it with multiple melodies that
            it had prompted earlier. We wanted to push the limits to see if it can generate different harmonies (chords
            perhaps) instead of just a singular melody</span></p>
    <p>We adapted our C code from lab 1 due to its similarity with what we were trying to achieve,
        although our code differs due to additional protothreads, using PWM instead of an ISR for DDS, not using a
        VGA display, and overall logic. We did not use any of Altera&rsquo;s IP, did not code in the public domain,
        did not need to deal with patent/trademark issues, and did not have to sign a non-disclosure agreement to
        get a sample part. There are certainly a myriad of future possibilities that combine LLMs with embedded
        devices that may lead to patent opportunities.</span></p>
    <h2>Appendix A</h2>
    <ul>
        <li>&quot;The group approves this report for inclusion on the
            course website.&quot;</li>
        <li>&quot;The group approves the video for inclusion on the course
            youtube channel.&quot;</li>
    </ul>
    <h2>Additional Appendices</h2>
    <h3>Primary Task Distribution</h3>
    <p>Both members contributed to all aspects of the project, these were the focuses
        of each team member:</p>
    <ul>
        <li>Edward: Ideation of project, ChatGPT python script, C script,
            debugging</li>
        <li>Filipe: Building the hardware setup, C script, debugging</li>
        </li>
    </ul>
    <h2>References</h2>
    <ul>
        <li>We used the ChatGPT API documentation to help us reliably
            communicate with ChatGPT through the terminal.</li>
        <li>We used the RP2040 datasheets and Lab1 as references.</li>
        </li>
        <li>Our PWM implementation is adapted from a fellow student,
            Pelham, who was also having trouble with DDS and was successful in using PWM.</li>
        <li>All materials used are from Lab1.</li>
    </ul>
</body>

</html>